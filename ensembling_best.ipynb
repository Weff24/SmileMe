{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qgYALkglJeVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb42cd2-b965-4a1a-8104-44b2b8fefd3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/low-resolution.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/stanford-dog-images.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "YjbGfdJCJevb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "UL8N5DTjJSXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LZ2Zl9wiFswk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import models\n",
        "\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Device"
      ],
      "metadata": {
        "id": "YwpokW_CIur8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "HYi90ZS1IzIT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-Processing"
      ],
      "metadata": {
        "id": "gmQGkdWpI1I6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4g-NEElzFswl"
      },
      "outputs": [],
      "source": [
        "# my_transform = transforms.Compose([\n",
        "#     transforms.Resize((100, 100)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Grayscale(),\n",
        "#     transforms.Normalize([0.485], [0.229])])\n",
        "\n",
        "# dataset = datasets.ImageFolder(\"../tmp/low-resolution\", my_transform)  # your dataset\n",
        "# train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "# valid_loader = DataLoader(valid_dataset, batch_size=50, shuffle=True, num_workers=2)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=50, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Normalize([0.485], [0.229])])\n",
        "\n",
        "dataset = datasets.ImageFolder(\"../tmp/low-resolution\", my_transform)\n",
        "\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(dataset.targets)),\n",
        "    test_size=0.4,\n",
        "    stratify=dataset.targets,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "val_indices, test_indices = train_test_split(\n",
        "    val_indices,\n",
        "    test_size=0.5,\n",
        "    stratify=[dataset.targets[i] for i in val_indices],\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "train_data = torch.utils.data.Subset(dataset, train_indices)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "valid_data = torch.utils.data.Subset(dataset, val_indices)\n",
        "valid_loader = DataLoader(valid_data, batch_size=32, num_workers=2)\n",
        "\n",
        "test_data = torch.utils.data.Subset(dataset, test_indices)\n",
        "test_loader = DataLoader(test_data, batch_size=32, num_workers=2)\n",
        "\n",
        "print(len(train_loader) * 32, len(valid_loader) * 32, len(test_loader) * 32)\n",
        "print((len(train_loader) * 32 + len(valid_loader) * 32 + len(test_loader) * 32))\n",
        "print(len(dataset))\n",
        "\n",
        "# train_features_raw, train_labels = next(iter(train_loader))\n",
        "# print(train_labels)\n",
        "# valid_features_raw, valid_labels = next(iter(valid_loader))\n",
        "# print(valid_labels)\n",
        "# test_features_raw, test_labels = next(iter(test_loader))\n",
        "# print(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1pm2Tu14akC",
        "outputId": "f98fd7c9-7ed1-4914-84df-ded83da47660"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42272 14112 14112\n",
            "70496\n",
            "70432\n",
            "tensor([115, 124, 125, 113,   4, 125,  92,   5,  29,   2, 122,  30,   2,  11,\n",
            "        119, 117,  58,  72,  71, 125,   1, 119, 128, 114,  92, 121, 119, 104,\n",
            "        121, 110,  74, 114])\n",
            "tensor([  1,  96, 119, 115,  83,  92, 127, 127,  96, 107,  11,  60,  95, 127,\n",
            "         32, 116, 104,  92, 114, 114, 114,  82,  91,  98,  58, 108,  92,  69,\n",
            "         19, 113,  22,  58])\n",
            "tensor([125, 129, 125, 106, 127,  71, 119, 124, 127, 127,  41,  33, 113, 127,\n",
            "         92,  37,   4, 113, 127,  58,   6,  34, 113, 114, 104, 119,  71, 119,\n",
            "          4,  92,  73,  20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "7l_ZSkD3JQLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "9g4JaYBEFswm"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,8,3,1)\n",
        "        self.conv2 = nn.Conv2d(8,16,3,1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(36864, 200)\n",
        "        self.fc2 = nn.Linear(200, 130)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.conv2(x)\n",
        "        x=F.relu(x)\n",
        "        x=F.max_pool2d(x,2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANN Model"
      ],
      "metadata": {
        "id": "nSUqs1xj-_2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10000, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 500)\n",
        "        self.fc3 = nn.Linear(500, 130)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        layer1 = F.relu(self.fc1(x))\n",
        "        layer2 = F.relu(self.fc2(layer1))\n",
        "        layer3 = self.fc3(layer2)\n",
        "        return layer3"
      ],
      "metadata": {
        "id": "0vN1Ou1d_BRv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFKcz-ezFswm"
      },
      "source": [
        "### Creating and Training Ensembling Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Ensembles"
      ],
      "metadata": {
        "id": "V_VbUbYBZomI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Ensemble"
      ],
      "metadata": {
        "id": "Qq-aFRkZakXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = []\n",
        "\n",
        "checkpoint1 = torch.load('50.pth')\n",
        "cnn_50_epochs = NeuralNet()\n",
        "cnn_50_epochs.load_state_dict(checkpoint1['model'])\n",
        "cnn_50_epochs = cnn_50_epochs.to(device)\n",
        "ensemble.append(cnn_50_epochs)\n",
        "\n",
        "checkpoint2 = torch.load('ann_40epochs_all_batched.pth')\n",
        "ann = MLP()\n",
        "ann.load_state_dict(checkpoint2['model'])\n",
        "ann = ann.to(device)\n",
        "ensemble.append(ann)\n",
        "\n",
        "checkpoint3 = torch.load('19.pth')\n",
        "cnn_20_epochs = NeuralNet()\n",
        "cnn_20_epochs.load_state_dict(checkpoint3['model'])\n",
        "cnn_20_epochs = cnn_20_epochs.to(device)\n",
        "ensemble.append(cnn_20_epochs)\n",
        "\n"
      ],
      "metadata": {
        "id": "bQo1THgHZ8Hc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test Ensemble Models"
      ],
      "metadata": {
        "id": "d59BAbmyb1G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predictions = []\n",
        "y_true = []\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    votes = []\n",
        "    \n",
        "    for model in ensemble:\n",
        "        log_probabilities = model(inputs)\n",
        "        y_prediction = log_probabilities.argmax(dim=1, keepdim=True)\n",
        "        votes.append(y_prediction)\n",
        "\n",
        "    # print(votes)\n",
        "    counter = Counter(votes)\n",
        "    # print(counter.most_common(1)[0][0])\n",
        "    # break\n",
        "    ensemble_y_prediction = counter.most_common(1)[0][0]\n",
        "\n",
        "    y_predictions.extend(list(np.concatenate(ensemble_y_prediction.tolist()).flat))\n",
        "    y_true.extend(labels.tolist())"
      ],
      "metadata": {
        "id": "JRPkJIGddamG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b061a8ab-0f43-4b84-f57a-9d4a20e70c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  2, 128,  70, 117, 125,  25,   0, 112,  95, 113, 114, 121, 125,  88,\n",
            "        127,  95,   1, 104, 126,  90,  58, 113, 105, 115,  98,   4,  92,  30,\n",
            "        119, 104, 113,  68,   4, 113, 122,  13, 115,  18, 127,  71,  66,   1,\n",
            "          9,  49, 101, 108,  77,  85, 121,   2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_true, y_predictions)"
      ],
      "metadata": {
        "id": "n5Iua2bbdagO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5852db3-ed0d-4157-9947-1702177f66b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9663495669459037"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Ensemble on Stanford Dogs Dataset"
      ],
      "metadata": {
        "id": "ebCBP2WMJa31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Processing"
      ],
      "metadata": {
        "id": "fbxnnFnl062l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanford_dog_list = os.listdir(\"../tmp/stanford-dog-images\")\n",
        "for i in range(len(stanford_dog_list)):\n",
        "    if \"-\" in stanford_dog_list[i]:\n",
        "        stanford_dog_list[i] = stanford_dog_list[i].lower()[stanford_dog_list[i].index(\"-\")+1::]\n",
        "\n",
        "print(len(stanford_dog_list))\n",
        "stanford_dog_list"
      ],
      "metadata": {
        "id": "q_c-aIbzwe9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsinghua_dog_list = os.listdir(\"../tmp/low-resolution\")\n",
        "for i in range(len(tsinghua_dog_list)):\n",
        "    if \"-\" in tsinghua_dog_list[i]:\n",
        "        tsinghua_dog_list[i] = tsinghua_dog_list[i].lower()[tsinghua_dog_list[i].rfind(\"-\")+1::]\n",
        "tsinghua_dog_list.remove(\"~$directoryfilecount.xlsx\")\n",
        "\n",
        "print(len(tsinghua_dog_list))\n",
        "tsinghua_dog_list"
      ],
      "metadata": {
        "id": "e-4yNqJdzDBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanford_to_tsinghua = {}\n",
        "for s_breed in stanford_dog_list:\n",
        "    if s_breed in tsinghua_dog_list:\n",
        "        stanford_to_tsinghua[stanford_dog_list.index(s_breed)] = tsinghua_dog_list.index(s_breed)\n",
        "\n",
        "stanford_to_tsinghua"
      ],
      "metadata": {
        "id": "WdyGh04X0tGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "WFa0MMDV08z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_transform = transforms.Compose([\n",
        "#     transforms.Resize((100, 100)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Grayscale(),\n",
        "#     transforms.Normalize([0.485], [0.229])])\n",
        "\n",
        "# stanford_dataset = datasets.ImageFolder(\"../tmp/stanford-dog-images\", my_transform)  # Stanford dogs dataset\n",
        "# train_dataset, valid_dataset, test_dataset = random_split(stanford_dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# # test_loader = DataLoader(orig_set, batch_size=100, shuffle=True, num_workers=2)\n",
        "# # test_features_raw, test_labels = next(iter(test_loader))\n",
        "\n",
        "# # test_features = test_features_raw.reshape(-1, 10000).squeeze()\n",
        "# # print(test_features.size())"
      ],
      "metadata": {
        "id": "yUJ3IIbhJqoG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Normalize([0.485], [0.229])])\n",
        "\n",
        "dataset = datasets.ImageFolder(\"../tmp/stanford-dog-images\", my_transform)\n",
        "\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(dataset.targets)),\n",
        "    test_size=0.4,\n",
        "    stratify=dataset.targets,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "val_indices, test_indices = train_test_split(\n",
        "    val_indices,\n",
        "    test_size=0.5,\n",
        "    stratify=[dataset.targets[i] for i in val_indices],\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "test_data = torch.utils.data.Subset(dataset, test_indices)\n",
        "test_loader = DataLoader(test_data, batch_size=32, num_workers=2)"
      ],
      "metadata": {
        "id": "Y6x6mBCt50xK"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predictions = []\n",
        "y_true = []\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    for label_i in range(len(labels)):\n",
        "        if labels[label_i].tolist() in stanford_to_tsinghua:\n",
        "            labels[label_i] = torch.tensor(stanford_to_tsinghua[labels[label_i].tolist()])\n",
        "    \n",
        "    votes = []\n",
        "    \n",
        "    for model in ensemble:\n",
        "        log_probabilities = model(inputs)\n",
        "        y_prediction = log_probabilities.argmax(dim=1, keepdim=True)\n",
        "        votes.append(y_prediction)\n",
        "\n",
        "    print(votes)\n",
        "    counter = Counter(votes)\n",
        "    print(counter.most_common(1)[0][0])\n",
        "    break\n",
        "    ensemble_y_prediction = counter.most_common(1)[0][0]\n",
        "\n",
        "    y_predictions.extend(list(np.concatenate(ensemble_y_prediction.tolist()).flat))\n",
        "    y_true.extend(labels.tolist())"
      ],
      "metadata": {
        "id": "1hpww3F4Jafe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_true, y_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlnKE_mb6e26",
        "outputId": "899ce340-a458-4e1a-af5e-81a92480bd7b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.007288629737609329"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "91613d3f5ca718a684ff9094a61c3901a8c82e744c47f88fcbc13decf1cdde28"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}